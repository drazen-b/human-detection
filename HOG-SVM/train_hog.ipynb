{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import pyramid_gaussian\n",
    "from skimage.io import imread\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import color\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import imutils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image # This will be used to read/modify images (can be done via OpenCV too)\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of HOG feature extraction\n",
    "orientations = 9\n",
    "pixels_per_cell = (8, 8)\n",
    "cells_per_block = (2, 2)\n",
    "threshold = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to images:\n",
    "\n",
    "pos_im_path = r\"../../datasets/kaggle_human01/human\" # This is the path of our positive input dataset\n",
    "# define the same for negatives\n",
    "neg_im_path= r\"../../datasets/kaggle_human01/no_human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559\n",
      "362\n"
     ]
    }
   ],
   "source": [
    "# read the image files:\n",
    "pos_im_listing = os.listdir(pos_im_path) # it will read all the files in the positive image path (so all the required images)\n",
    "neg_im_listing = os.listdir(neg_im_path)\n",
    "num_pos_samples = size(pos_im_listing) # simply states the total no. of images\n",
    "num_neg_samples = size(neg_im_listing)\n",
    "print(num_pos_samples) # prints the number value of the no.of samples in positive dataset\n",
    "print(num_neg_samples)\n",
    "data= []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute HOG features and label them:\n",
    "\n",
    "for file in pos_im_listing: #this loop enables reading the files in the pos_im_listing variable one by one\n",
    "    img = Image.open(pos_im_path + '/' + file) # open the file\n",
    "    img = img.resize((256, 256))\n",
    "    gray = img.convert('L') # convert the image into single channel i.e. RGB to grayscale\n",
    "    # calculate HOG for positive features\n",
    "    fd = hog(gray, orientations, pixels_per_cell, cells_per_block, block_norm='L2', feature_vector=True)# fd= feature descriptor\n",
    "    data.append(fd)\n",
    "    labels.append(1)\n",
    "\n",
    "type(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the negative images\n",
    "for file in neg_im_listing:\n",
    "    img= Image.open(neg_im_path + '/' + file)\n",
    "    img = img.resize((256, 256))\n",
    "    gray= img.convert('L')\n",
    "    # Now we calculate the HOG for negative features\n",
    "    fd = hog(gray, orientations, pixels_per_cell, cells_per_block, block_norm='L2', feature_vector=True) \n",
    "    data.append(fd)\n",
    "    labels.append(0)\n",
    "# encode the labels, converting them from strings to integers\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921, 34596)\n",
      "<class 'numpy.ndarray'>\n",
      " Constructing training/testing split...\n",
      "(736, 34596)\n",
      "(736,)\n",
      " Training Linear SVM classifier...\n",
      " Evaluating classifier on test data ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.51      0.55        72\n",
      "           1       0.72      0.78      0.75       113\n",
      "\n",
      "    accuracy                           0.68       185\n",
      "   macro avg       0.66      0.65      0.65       185\n",
      "weighted avg       0.67      0.68      0.67       185\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drazen/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Partitioning the data into training and testing splits, using 80%\n",
    "# of the data for training and the remaining 20% for testing\n",
    "np.stack(data, axis=0)\n",
    "print(shape(data))\n",
    "print(type(labels))\n",
    "\n",
    "print(\" Constructing training/testing split...\")\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(data, dtype=object), labels, test_size=0.20, random_state=42)\n",
    "\n",
    "print(shape(trainData))\n",
    "print(shape(trainLabels))\n",
    "\n",
    "# %% Train the linear SVM\n",
    "print(\" Training Linear SVM classifier...\")\n",
    "model = LinearSVC()\n",
    "model.fit(trainData, trainLabels)\n",
    "#%% Evaluate the classifier\n",
    "print(\" Evaluating classifier on test data ...\")\n",
    "predictions = model.predict(testData)\n",
    "print(classification_report(testLabels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hog_model.npy']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model:\n",
    "#%% Save the Model\n",
    "joblib.dump(model, 'hog_model.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hog-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5738405e9b45f44e1c8e67433f30d0aae78233b969c0822e5dec553fb5faf5a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
